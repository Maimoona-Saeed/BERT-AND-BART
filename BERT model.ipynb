{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "493aefa4",
      "metadata": {
        "id": "493aefa4"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1e8dbe1",
      "metadata": {
        "id": "c1e8dbe1"
      },
      "source": [
        "### What is BERT? (Bidirectional Encoder Representations from Transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67a8517f",
      "metadata": {
        "id": "67a8517f"
      },
      "source": [
        "***BERT is a deep learning model developed by Google in 2018.***  \n",
        "  \n",
        "***It is based on the Transformer architecture.***  \n",
        "  \n",
        "***It reads entire sentences at once, from both left and right â€” this is called bidirectional.***\n",
        "\n",
        "For more information, the original paper can be found [here](https://arxiv.org/abs/1810.04805).\n",
        "\n",
        "[HuggingFace documentation](https://huggingface.co/transformers/model_doc/bert.html)\n",
        "\n",
        "[Bert documentation](https://characters.fandom.com/wiki/Bert_(Sesame_Street) ;)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05d5163e",
      "metadata": {
        "id": "05d5163e"
      },
      "source": [
        "# Exploratory Data Analysis and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3386cb6",
      "metadata": {
        "id": "c3386cb6"
      },
      "source": [
        "We will use the SMILE Twitter dataset.\n",
        "\n",
        "_Wang, Bo; Tsakalidis, Adam; Liakata, Maria; Zubiaga, Arkaitz; Procter, Rob; Jensen, Eric (2016): SMILE Twitter Emotion dataset. figshare. Dataset. https://doi.org/10.6084/m9.figshare.3187909.v2_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b191d9e",
      "metadata": {
        "id": "4b191d9e"
      },
      "outputs": [],
      "source": [
        "import torch # Imports the PyTorch library which is a popular deep learning framework used for building and training neural networks.\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fccf97c4",
      "metadata": {
        "id": "fccf97c4"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('smileannotationsfinal.csv', names=['id', 'text', 'category'])\n",
        "df.set_index('id', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f9a327",
      "metadata": {
        "id": "94f9a327"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "723219c7",
      "metadata": {
        "id": "723219c7"
      },
      "outputs": [],
      "source": [
        "df.category.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d101baa",
      "metadata": {
        "id": "0d101baa"
      },
      "outputs": [],
      "source": [
        "df = df[~df.category.str.contains('\\|')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ac66407",
      "metadata": {
        "id": "2ac66407"
      },
      "outputs": [],
      "source": [
        "df = df[df.category != 'nocode']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05ac9f1c",
      "metadata": {
        "id": "05ac9f1c"
      },
      "outputs": [],
      "source": [
        "df.category.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07242d46",
      "metadata": {
        "id": "07242d46"
      },
      "outputs": [],
      "source": [
        "possible_labels = df.category.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f73523f",
      "metadata": {
        "id": "5f73523f"
      },
      "outputs": [],
      "source": [
        "label_dict = {}\n",
        "for index, possible_label in enumerate(possible_labels):\n",
        "    label_dict[possible_label] = index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb06ad44",
      "metadata": {
        "id": "cb06ad44"
      },
      "outputs": [],
      "source": [
        "label_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "572e2863",
      "metadata": {
        "id": "572e2863"
      },
      "outputs": [],
      "source": [
        "df['label'] = df.category.replace(label_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0fd4f35",
      "metadata": {
        "id": "b0fd4f35"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21623ff0",
      "metadata": {
        "id": "21623ff0"
      },
      "source": [
        "# Training/Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d5c13a9",
      "metadata": {
        "id": "6d5c13a9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e1f3d47",
      "metadata": {
        "id": "7e1f3d47"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(df.index.values,\n",
        "                                                  df.label.values,\n",
        "                                                  test_size=0.15,\n",
        "                                                  random_state=17,\n",
        "                                                  stratify=df.label.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a4d073",
      "metadata": {
        "id": "24a4d073"
      },
      "outputs": [],
      "source": [
        "df['data_type'] = ['not_set']*df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71c6e70e-cc57-4af1-95ad-bbcdbe531ddf",
      "metadata": {
        "id": "71c6e70e-cc57-4af1-95ad-bbcdbe531ddf"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a058ff",
      "metadata": {
        "id": "61a058ff"
      },
      "outputs": [],
      "source": [
        "df.loc[X_train, 'data_type'] = 'train'\n",
        "df.loc[X_val, 'data_type'] = 'val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "326891ac",
      "metadata": {
        "id": "326891ac"
      },
      "outputs": [],
      "source": [
        "df.groupby(['category', 'label', 'data_type']).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5dc3d3d",
      "metadata": {
        "id": "c5dc3d3d"
      },
      "source": [
        "# Loading Tokenizer and Encoding our Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da8fdb95",
      "metadata": {
        "id": "da8fdb95"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d78de2",
      "metadata": {
        "id": "a7d78de2"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db25e3b2",
      "metadata": {
        "id": "db25e3b2"
      },
      "outputs": [],
      "source": [
        "#uses a BERT tokenizer to convert text data into numerical\n",
        "encoded_data_train = tokenizer.batch_encode_plus(\n",
        "    df[df.data_type=='train'].text.values,\n",
        "    add_special_tokens=True,\n",
        "    return_attention_mask=True,\n",
        "    pad_to_max_length=True,\n",
        "    max_length=256,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "encoded_data_val = tokenizer.batch_encode_plus(\n",
        "    df[df.data_type=='val'].text.values,\n",
        "    add_special_tokens=True,\n",
        "    return_attention_mask=True,\n",
        "    pad_to_max_length=True,\n",
        "    max_length=256,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "\n",
        "input_ids_train = encoded_data_train['input_ids']\n",
        "attention_masks_train = encoded_data_train['attention_mask']\n",
        "labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n",
        "\n",
        "input_ids_val = encoded_data_val['input_ids']\n",
        "attention_masks_val = encoded_data_val['attention_mask']\n",
        "labels_val = torch.tensor(df[df.data_type=='val'].label.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "822af436",
      "metadata": {
        "id": "822af436"
      },
      "outputs": [],
      "source": [
        "encoded_data_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3119c70",
      "metadata": {
        "id": "d3119c70"
      },
      "outputs": [],
      "source": [
        "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
        "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a3df2ba",
      "metadata": {
        "id": "2a3df2ba"
      },
      "outputs": [],
      "source": [
        "len(dataset_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7a8fd0c",
      "metadata": {
        "id": "a7a8fd0c"
      },
      "outputs": [],
      "source": [
        "len(dataset_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba7e2fb2",
      "metadata": {
        "id": "ba7e2fb2"
      },
      "source": [
        "# Setting up BERT Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4de45d92",
      "metadata": {
        "id": "4de45d92"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BertForSequenceClassification is built on top of BertModel. The BertForSequenceClassification effectively takes the BertModel and builds a classification extra layer. Import this library from transfomer which is used to convert the input data into a format that can be processed by the model.**"
      ],
      "metadata": {
        "id": "q4YlMJL_H7jM"
      },
      "id": "q4YlMJL_H7jM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a79ea0e1",
      "metadata": {
        "id": "a79ea0e1"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels=len(label_dict),\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This line loads a pre-trained BERT model for classifying text into categories. It uses the bert-base-uncased version, which ignores letter casing like uppercase and lowercase.\n",
        "num_labels sets how many classes the model will predict.\n",
        "It turns off extra outputs like attention and hidden states to save memory and speed up processing.**"
      ],
      "metadata": {
        "id": "o1yOdks5J8uF"
      },
      "id": "o1yOdks5J8uF"
    },
    {
      "cell_type": "markdown",
      "id": "3bca77c9-7093-44bc-8442-eb47c266649e",
      "metadata": {
        "id": "3bca77c9-7093-44bc-8442-eb47c266649e"
      },
      "source": [
        "# Creating Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4232e119",
      "metadata": {
        "id": "4232e119"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The dataloader facilitates efficient loading and processing of data, particularly for machine learning models.**\n",
        "\n",
        "**Random sampler samples elements from the dataset randomly, without replacement, ensuring that each element is selected only once during an epoch.**\n",
        "\n",
        "**Sequential Sampler iterates through the dataset indices in a sequential manner, starting from the first element and moving to the next until the end.**"
      ],
      "metadata": {
        "id": "lRO-EqeRK-3r"
      },
      "id": "lRO-EqeRK-3r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "534f7826",
      "metadata": {
        "id": "534f7826"
      },
      "outputs": [],
      "source": [
        "#set up data loaders to efficiently feed data to the model in batches during training and validation.\n",
        "batch_size = 32\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train,\n",
        "                              sampler=RandomSampler(dataset_train),\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "dataloader_validation = DataLoader(dataset_val,\n",
        "                                   sampler=SequentialSampler(dataset_val),\n",
        "                                   batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**These lines create data loaders to feed data into the model during training and validation.\n",
        "Batch size means that the model will process 32 samples at a time.\n",
        "dataloader_train randomly shuffles training data using RandomSampler.\n",
        "dataloader_validation reads validation data in order using SequentialSampler.**"
      ],
      "metadata": {
        "id": "weISl_eDMPLc"
      },
      "id": "weISl_eDMPLc"
    },
    {
      "cell_type": "markdown",
      "id": "74ca870b-8920-4b73-8141-cbf721cf9f0b",
      "metadata": {
        "id": "74ca870b-8920-4b73-8141-cbf721cf9f0b"
      },
      "source": [
        "# Setting Up Optimiser and Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f732a28d",
      "metadata": {
        "id": "f732a28d"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The AdamW will adjust the model's weight to reduce the error.**\n",
        "\n",
        " **get_linear_schedule_with_warmup slowly increases the learning rate at the start (warmup), then decreases it, helping the model learn more smoothly.**"
      ],
      "metadata": {
        "id": "EW2r76kHN4GR"
      },
      "id": "EW2r76kHN4GR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1552d29",
      "metadata": {
        "id": "b1552d29"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=1e-5,\n",
        "                  eps=1e-8)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This line initializes the AdamW optimizer, which is used to update the model's weights during training to minimize the error. It takes the model's parameters as input, meaning it knows which parts of the model to adjust. The learning rate is set to a small value of 1e-5, which controls how quickly the model learns. Smaller values make learning slower and more stable. The eps=1e-8 is a small number added to avoid division by zero or very small numbers during optimization, ensuring numerical stability.**"
      ],
      "metadata": {
        "id": "uBFoHjNTOfcT"
      },
      "id": "uBFoHjNTOfcT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d1fe739",
      "metadata": {
        "id": "3d1fe739"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(dataloader_train)*epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The first line initializes the interation to 3. We use adam optmizer as an optimizer. num_warmup_steps=0 means there's no warmup period, so the learning rate starts decreasing right away. The num_training_steps is set to the total number of batches the model will see during training, calculated by multiplying the number of batches per epoch (len(dataloader_train)) by the total number of epochs. This helps the learning rate decrease gradually as training progresses.**"
      ],
      "metadata": {
        "id": "TpAQJ9_jPNmE"
      },
      "id": "TpAQJ9_jPNmE"
    },
    {
      "cell_type": "markdown",
      "id": "d5c8165c-8233-4649-b8be-02be217c7d21",
      "metadata": {
        "id": "d5c8165c-8233-4649-b8be-02be217c7d21"
      },
      "source": [
        "# Defining our Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5bf8e32",
      "metadata": {
        "id": "d5bf8e32"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**F1 score is a machine learning evaluation metric that measures a model's accuracy.\n",
        "Numpy is used to work with n dimensional arrays and lists.**"
      ],
      "metadata": {
        "id": "Fymc0kPMSVdI"
      },
      "id": "Fymc0kPMSVdI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94fc5efa",
      "metadata": {
        "id": "94fc5efa"
      },
      "outputs": [],
      "source": [
        "#calculates the weighted F1 score by comparing the model's predicted class\n",
        "def f1_score_func(preds, labels):\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return f1_score(labels_flat, preds_flat, average='weighted')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This function calculates the F1 score, which measures how well the model is performing. It first finds the predicted class with the highest score using argmax. Then, it flattens both predictions and true labels to 1D arrays for comparison. Finally, it returns the weighted F1 score, which balances precision and recall based on label importance.**"
      ],
      "metadata": {
        "id": "_7yJTvGuS5Rj"
      },
      "id": "_7yJTvGuS5Rj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8df2a6c",
      "metadata": {
        "id": "c8df2a6c"
      },
      "outputs": [],
      "source": [
        "#calculates and prints the prediction accuracy for each individual class in your dataset\n",
        "def accuracy_per_class(preds, labels):\n",
        "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
        "\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    for label in np.unique(labels_flat):\n",
        "        y_preds = preds_flat[labels_flat==label]\n",
        "        y_true = labels_flat[labels_flat==label]\n",
        "        print(f'Class: {label_dict_inverse[label]}')\n",
        "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This function prints the accuracy for each class separately.\n",
        "First, it creates a dictionary to map label numbers back to their names.\n",
        "It flattens the predictions and true labels to make them easier to compare.\n",
        "Then, for each unique class, it finds how many predictions were correct out of the total for that class.\n",
        "It prints the class name and its accuracy in the format: correct predictions / total samples.**"
      ],
      "metadata": {
        "id": "At_-1afBTU-J"
      },
      "id": "At_-1afBTU-J"
    },
    {
      "cell_type": "markdown",
      "id": "a615e3d8-8ce5-488f-af54-d2f3839d2ae6",
      "metadata": {
        "id": "a615e3d8-8ce5-488f-af54-d2f3839d2ae6"
      },
      "source": [
        "# Creating our Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a27ff345",
      "metadata": {
        "id": "a27ff345"
      },
      "outputs": [],
      "source": [
        "#andom seeds for reproducibility in Python, NumPy, and PyTorch, ensuring consistent results from random operations.\n",
        "import random\n",
        "\n",
        "seed_val = 17\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**These lines set a fixed random seed to make the results reproducible.\n",
        "By setting seed_val = 17, it ensures that random operations (like shuffling or weight initialization) give the same results every time.\n",
        "It sets the seed for Python random module, NumPy, and PyTorch, so everything behaves consistently during each run.**"
      ],
      "metadata": {
        "id": "Ja6ys1MdTY79"
      },
      "id": "Ja6ys1MdTY79"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3332b11d",
      "metadata": {
        "id": "3332b11d"
      },
      "outputs": [],
      "source": [
        "#sets the computing device for the model to either a GPU (if available) or the CPU and then prints the selected device.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If cuda is available in torch.cuda.is_available method that use cuda else use cpu. Then print the device to see which device is used.**"
      ],
      "metadata": {
        "id": "JVCwKFK7Vyny"
      },
      "id": "JVCwKFK7Vyny"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a4af67b-c05f-47c1-b4f8-7f171ff4a989",
      "metadata": {
        "id": "2a4af67b-c05f-47c1-b4f8-7f171ff4a989"
      },
      "outputs": [],
      "source": [
        "# function evaluates the model on validation data, calculates the average loss, and collects predictions and t\n",
        "def evaluate(dataloader_val):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    loss_val_total = 0\n",
        "    predictions, true_vals = [], []\n",
        "\n",
        "    for batch in dataloader_val:\n",
        "\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "\n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2],\n",
        "                 }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        loss_val_total += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = inputs['labels'].cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "        true_vals.append(label_ids)\n",
        "\n",
        "    loss_val_avg = loss_val_total/len(dataloader_val)\n",
        "\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    true_vals = np.concatenate(true_vals, axis=0)\n",
        "\n",
        "    return loss_val_avg, predictions, true_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This function evaluates the models performance on the validation data.\n",
        "It sets the model to evaluation mode using model.eval() to turn off dropout and other training-only behaviors.\n",
        "For each batch in the validation set, it moves data to the correct device CPU or GPU and prepares the input.\n",
        "It then runs the model without updating weights (torch.no_grad()), collects the loss, predictions, and true labels.\n",
        "Finally, it calculates the average loss and returns it along with all predictions and true labels for further analysis.**"
      ],
      "metadata": {
        "id": "ViZqLso5WXQy"
      },
      "id": "ViZqLso5WXQy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fe85ecb-7b94-4bb0-a1fd-13606ea2efe8",
      "metadata": {
        "id": "7fe85ecb-7b94-4bb0-a1fd-13606ea2efe8"
      },
      "outputs": [],
      "source": [
        "#BERT model for a specified number of epochs, processes data in batches, updates model weights based on calculated loss, saves the model state after each epoch, and evaluates performance on validation data.\n",
        "for epoch in tqdm(range(1, epochs+1)):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    loss_train_total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
        "    for batch in progress_bar:\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "\n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2],\n",
        "                 }\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        loss_train_total += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
        "\n",
        "\n",
        "    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n",
        "\n",
        "    tqdm.write(f'\\nEpoch {epoch}')\n",
        "\n",
        "    loss_train_avg = loss_train_total/len(dataloader_train)\n",
        "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
        "\n",
        "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
        "    val_f1 = f1_score_func(predictions, true_vals)\n",
        "    tqdm.write(f'Validation loss: {val_loss}')\n",
        "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This code trains the model for multiple epochs with progress bars to show real-time updates.\n",
        "For each epoch, it sets the model to training mode and initializes the total training loss.\n",
        "It loops through batches in the training data, moves data to the device, and computes the loss.\n",
        "After calculating gradients with loss.backward(), it clips them to prevent very large updates, then updates model weights with the optimizer and adjusts the learning rate using the scheduler.\n",
        "The progress bar shows the current training loss per batch. After each epoch, the models state is saved to a file.\n",
        "Finally, it prints the average training loss, evaluates the model on validation data, and prints the validation loss and weighted F1 score to track performance.**"
      ],
      "metadata": {
        "id": "88hvCGPZXZAh"
      },
      "id": "88hvCGPZXZAh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8089c147-03fc-44bf-b9fb-9a2726678253",
      "metadata": {
        "id": "8089c147-03fc-44bf-b9fb-9a2726678253"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels=len(label_dict),\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)\n",
        "\n",
        "model.to(device) #pre-trained BERT model for text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This code loads a pre-trained BERT model designed for text classification using the \"bert-base-uncased\" version, which treats uppercase and lowercase letters the same. It sets the number of output classes based on your label dictionary and disables extra outputs like attention scores and hidden states to keep things simple. Finally, it moves the model to the specified device (CPU or GPU) so it can run there efficiently.**"
      ],
      "metadata": {
        "id": "n4ipmiS0XjZC"
      },
      "id": "n4ipmiS0XjZC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ac8279-e488-4406-aad9-b6022d46a0b4",
      "metadata": {
        "id": "83ac8279-e488-4406-aad9-b6022d46a0b4"
      },
      "outputs": [],
      "source": [
        "_, predictions, true_vals = evaluate(dataloader_validation) #evaluates the model on the validation data and stores the predictions and true labels for later analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This line runs the evaluate function on the validation data to check how well the model is performing.**\n",
        "\n",
        "**It returns three values: the average validation loss, the models predicted outputs (predictions), and the actual labels (true_vals).\n",
        "These predictions and true values can then be used to calculate metrics like accuracy or F1 score.**"
      ],
      "metadata": {
        "id": "_bXQ9atJXsXE"
      },
      "id": "_bXQ9atJXsXE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3823a18a-587d-408b-88bb-57b7f96e04a8",
      "metadata": {
        "id": "3823a18a-587d-408b-88bb-57b7f96e04a8"
      },
      "outputs": [],
      "source": [
        "accuracy_per_class(predictions, true_vals) #This line calculates and prints the accuracy of the model for each category in the validation data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This line calls the accuracy_per_class function to measure how accurately the model predicted each class.\n",
        "It compares the model's predictions (predictions) with the actual labels (true_vals) and prints the accuracy for each class individually.**"
      ],
      "metadata": {
        "id": "Jky0sYdfYXOM"
      },
      "id": "Jky0sYdfYXOM"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}